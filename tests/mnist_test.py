# -*- coding: utf-8 -*-

"""NEMO: post-training quantization MNIST example

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AmcITfN2ELQe07WKQ9szaxq-WSu4hdQb

This example guides a first-time user of NEMO into the NEMO quantization process, using a small pretrained network and going through post-training per-layer quantization (i.e., representing weight and activation tensors as integers) and deployment (i.e., organizing operations so that they are an accurate representation of behavior in integer-based hardware). We will see how this operates through four stages: *FullPrecision*, *FakeQuantized*,*QuantizedDeployable*, *IntegerDeployable*.

NEMO uses `float32` tensors to represent data at all four stages - including *IntegerDeployable*. This means that NEMO code does not need special hardware support for integers to run on GPUs. It also means that NEMO is not (and does not want to be) a runtime for quantized neural networks on embedded systems!

Let us start by installing dependencies...
"""

"""... and import all packages, including NEMO itself:"""

import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import nemo
from tqdm import tqdm

"""The first real step is to define the network topology. This works exactly like in a "standard" PyTorch script, using regular `torch.nn.Module` instances.  NEMO can transform many layers defined in `torch.nn` into its own representations. There are a few constraints, however, to the network construction:
* Use `torch.nn.Module`, not `torch.autograd.Function`: NEMO works by listing modules and ignores functions by construction. Everything coming from the `torch.nn.functional` library is ignored by NEMO: for example, you have to use `torch.nn.ReLU` module instead of the equivalent `torch.nn.functional.relu` function, which is often found in examples online.
* Instantiate a separate `torch.nn.Module` for each node in your topology; you already do this for parametric modules (e.g., `torch.nn.Conv2d`), but you have to do the same also for `torch.nn.ReLU` and other parameterless modules. NEMO will introduce quantization parameters that will change along the network.
* To converge two network branches (e.g., a main and a residual branch), a normal PyTorch network will usually add the values of their output tensors. This will keep working for a network at the *FakeQuantized* stage, i.e., one that can be fine-tuned keeping into account quantization - but it will break in later stages. In the *QuantizedDeployable* and *IntegerDeployable* stages, the branch reconvergence has to take into account the possibly different precision of the two branches, therefore NEMO has to know that there is an "Add" node at that point of the network. This can be realized using the `nemo.quant.pact.PACT_IntegerAdd` module, which is entirely equivalent to a normal addition in *FullPrecision* and *FakeQuantized* stages.
"""

class ExampleNet(nn.Module):
    def __init__(self):
        super(ExampleNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.bn1 = nn.BatchNorm2d(32)
        self.relu1 = nn.ReLU() # <== Module, not Function!
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.bn2 = nn.BatchNorm2d(64)
        self.relu2 = nn.ReLU() # <== Module, not Function!
        self.pool2 = nn.MaxPool2d(2)
        self.fc1 = nn.Linear(9216, 256)
        self.fcrelu1 = nn.ReLU() # <== Module, not Function!
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x) # <== Module, not Function!
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu2(x) # <== Module, not Function!
        x = self.pool2(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = self.fcrelu1(x) # <== Module, not Function!
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1) # <== the softmax operation does not need to be quantized, we can keep it as it is
        return output

"""Then we define the training and testing functions (MNIST has no validation set). These are essentially identical to regular PyTorch code, with only one difference: testing (and validation) functions have a switch to support the production of non-negative integer data. This is important to test the last stage of quantization, i.e., *IntegerDeployable*. Of course, this change might also be effectively performed inside the data loaders; in this example, we use standard `torchvision` data loaders for MNIST."""

# convenience class to keep track of averages
class Metric(object):
    def __init__(self, name):
        self.name = name
        self.sum = torch.tensor(0.)
        self.n = torch.tensor(0.)
    def update(self, val):
        self.sum += val.cpu()
        self.n += 1
    @property
    def avg(self):
        return self.sum / self.n

def train(model, device, train_loader, optimizer, epoch, verbose=False):
    model.train()
    train_loss = Metric('train_loss')
    with tqdm(total=len(train_loader),
          desc='Train Epoch     #{}'.format(epoch + 1),
          disable=not verbose) as t:
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()
            train_loss.update(loss)
            t.set_postfix({'loss': train_loss.avg.item()})
            t.update(1)
    return train_loss.avg.item()

def test(model, device, test_loader, integer=False, verbose=False):
    model.eval()
    test_loss = 0
    correct = 0
    test_acc = Metric('test_acc')
    with tqdm(total=len(test_loader),
          desc='Test',
          disable=not verbose) as t:
        with torch.no_grad():
            for data, target in test_loader:
                if integer:      # <== this will be useful when we get to the 
                    data *= 255  #     IntegerDeployable stage
                data, target = data.to(device), target.to(device)
                output = model(data)
                test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
                pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability
                correct += pred.eq(target.view_as(pred)).sum().item()
                test_acc.update((pred == target.view_as(pred)).float().mean())
                t.set_postfix({'acc' : test_acc.avg.item() * 100. })
                t.update(1)
    test_loss /= len(test_loader.dataset)
    return test_acc.avg.item() * 100.

"""Set up the dataset loaders."""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([
        transforms.ToTensor()
    ])),
    batch_size=128, shuffle=True, **kwargs
)
test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=False, transform=transforms.Compose([
        transforms.ToTensor()
    ])),
    batch_size=128, shuffle=False, **kwargs
)

"""Download a pretrained model, and test it! Here we operate at what we call the ***FullPrecision*** stage: the regular PyTorch representation, which relies on real-valued tensors represented by `float32` in your CPU/GPU."""

# !wget https://raw.githubusercontent.com/FrancescoConti/nemo_examples_helper/master/mnist_cnn_fp.pt
model = ExampleNet().to(device)
state_dict = torch.load("mnist_cnn_fp.pt", map_location='cpu')
model.load_state_dict(state_dict, strict=True)
acc = test(model, device, test_loader)
print("\nFullPrecision accuracy: %.02f%%" % acc)
assert acc >= 99.0

"""Now, it's time to try out some post-training quantization. We will do so by switching to the ***FakeQuantized*** stage. This representation is very similar to *FullPrecision*, as it still uses real-valued tensors for weights and activations. However, activation functions such as ReLU become quantization functions, imposing that the output is representable in a certain number of steps. Mathematically,
$$
    \mathbf{y} = \mathrm{ReLU}(\mathbf{x}) = \mathrm{clip}_{ [0,\infty) }(\mathbf{x}) \quad\longrightarrow\quad \mathbf{y} = \left\lfloor 1/\varepsilon \cdot \mathrm{clip}_{ [0,\alpha) } (\mathbf{x}) \right\rfloor \cdot \varepsilon
$$

Here, we introduce two changes to the ReLU. First, the clipping function is not only clipping at 0, but also at a maximum value $\alpha$, which can be set to the maximum value of $\mathbf{y}$ in the *FullPrecision* stage (see later). Second, we introduce a *quantum* $\varepsilon$, which is the smalles real-valued amount that is representable in $\mathbf{y}$. With $Q$ bits, $\varepsilon = \alpha / (2^Q - 1)$.
Network weights are passed through a similar function before being used, with the main difference that they are not strictly non-negative: they are clipped between two values $\alpha$ and $\beta$.

At this stage, the network can be trained / fine-tuned (see details in the fine-tuning example). Also, the numerical values of activations can differ a bit from a real hardware implementation. This is because quantization of tensors is only actively imposed in the activation functions, but not in the other operations (i.e., in ReLU, not in BatchNorm2d and Conv2d).

Here, we first transform the model in a *FakeQuantized* version targeting a very relaxed 16-bit quantization for weights and activations. The quantization for each layer can be tweaked by means of a precision dictionary; notice that for weights we actually impose 15 bits instead of 16: this is to take into account the asymmetricity of $\alpha$ and $\beta$.
"""

model = nemo.transform.quantize_pact(model, dummy_input=torch.randn((1,1,28,28)).to(device))
precision = {
    'conv1': {
        'W_bits' : 15
    },
    'conv2': {
        'W_bits' : 15
    },
    'fc1': {
        'W_bits' : 15
    },
    'fc2': {
        'W_bits' : 15
    },
    'relu1': {
        'x_bits' : 16
    },
    'relu2': {
        'x_bits' : 16
    },
    'fcrelu1': {
        'x_bits' : 16
    },
}
model.change_precision(bits=1, min_prec_dict=precision)
acc = test(model, device, test_loader)
print("\nFakeQuantized @ 16b accuracy (first try): %.02f%%" % acc)
assert acc >= 80.0

"""The first try looks... not so good. 82% is actually pretty bad for MNIST! What happened? Remember that while clipping parameters for weights can be set statically, this is not true for activations: so the missing piece is the characterization of activation clipping ($\alpha$ parameter), which is currently set to a default value.

In NEMO, this initial calibration can be performed by setting a special *statistics collection* mode for activations, which is used to explicitly reset the $\alpha$ params. The calibration is performed directly by running inference over a dataset; in this case, we cheat a bit and do it on the test set.
"""

model.set_statistics_act()
_ = test(model, device, test_loader)
model.unset_statistics_act()
model.reset_alpha_act()
acc = test(model, device, test_loader)
print("\nFakeQuantized @ 16b accuracy (calibrated): %.02f%%" % acc)
assert acc >= 99.0

"""Now the accuracy is substantially the same as the initial one! This is what we expect to see using a very conservative quantization scheme with 16 bits. Due to the way that NEMO implements the *FakeQuantized* stage, it is very easy to explore what happens by imposing a stricter or mixed precision quantization scheme. The number of bits we can use is very free: we can even set it to "fractionary" values if we want, which corresponds to intermediate $\varepsilon$ *quantum* sizes with respect to the nearest integers. For example, let's force `conv1`, `conv2` and `fc1` to be 7 bits, `fc2` to use only 3 bits for its parameters, and all activations to be 8-bit.

After the experiment, we also save the network in a PyTorch checkpoint file, because afterwards we'll start doing some destructive transformations...
"""

precision = {
    'conv1': {
        'W_bits' : 7
    },
    'conv2': {
        'W_bits' : 7
    },
    'fc1': {
        'W_bits' : 7
    },
    'fc2': {
        'W_bits' : 3
    },
    'relu1': {
        'x_bits' : 8
    },
    'relu2': {
        'x_bits' : 8
    },
    'fcrelu1': {
        'x_bits' : 8
    },
}
model.change_precision(bits=1, min_prec_dict=precision)
acc = test(model, device, test_loader)
print("\nFakeQuantized @ mixed-precision accuracy: %.02f%%" % acc)
assert acc >= 99.0
nemo.utils.save_checkpoint(model, None, 0, checkpoint_name='mnist_fq_mixed')

"""Since MNIST is very easy, there is only a very small accuracy reduction despite the aggressive reduction at the end of the network. Now, let us progress towards a possible deployment. One of the possible steps is the so-called *folding* of batch-normalization layers. With this operation, the normalization performed by batch-norm layers is absorbed within the parameters of the convolutional layers. To perform it, we first do the folding itself, then we reset the clipping parameters of weights (because the weights change their value!)."""

model.fold_bn()
model.reset_alpha_weights()
acc = test(model, device, test_loader)
print("\nFakeQuantized @ mixed-precision (folded) accuracy: %.02f%%" % acc)
assert acc >= 98.8

"""Notice a small reduction in accuracy: as you might remember, the batch-norm layers were not quantized before folding; folding absorbs them inside the quantized parameters. Therefore a small reduction is expected! There are also a few techniques that can be used to recover accuracy, such as the weight equaliztion for Data Free Quantization proposed by Nagel et al. (https://arxiv.org/abs/1906.04721) . Here we try it on our network, which requires also a new calibration pass."""

model.equalize_weights_dfq({'conv1':'conv2'}, reset_alpha=False)
model.set_statistics_act()
_ = test(model, device, test_loader)
model.unset_statistics_act()
model.reset_alpha_act()
acc = test(model, device, test_loader)
print("\nFakeQuantized @ mixed-precision (folded+equalized) accuracy: %.02f%%" % acc)
assert acc >= 99.0

"""Now we go back one step, reloading the state from the saved checkpoint (before folding) to show the "standard" deployment strategy that we use, based on Integer Batch-Norm (Rusci et al., https://arxiv.org/abs/1905.13082). 
This is organized in two steps: first, we replace all `BatchNorm2d` in the network into a special quantized form, which is equivalent to freezing their parameters and transforms them, essentially, in channel-wise affine transforms. Then, we harden weights in their current quantum representation. Finally, we use the `set_deployment` method to bring the network to the ***QuantizedDeployable*** stage.

In the *QuantizedDeployable* stage, the network is still using real-valued weights and activations; but all operations consume and produce quantized tensors that can always be decomposed in the product of a real-valued quantum with an integer tensor, which we call the *integer image*. The network cannot be trained any more, but it can be exported in an ONNX graph that faithfully represents quantization.

Bringing the network to this stage requires setting an input quantum $\varepsilon_{in}$, which corresponds to the value of 1 bit in the integer representation of our input. For example, for an 8-bit image represented by a tensor in the range $[0,1]$, this will be $1/255$.
Internally, NEMO uses a graph representation of the network to propagate the $\varepsilon$ to all layers that are not explicitly quantized (in practice, all layers that are not activations).

In the next cell, we bring our MNIST network to this representation, then we test it to verify that it still works.
"""

state_dict = torch.load('checkpoint/mnist_fq_mixed.pth')['state_dict']
model.load_state_dict(state_dict, strict=True)
model = nemo.transform.bn_quantizer(model)
model.harden_weights()
model.set_deployment(eps_in=1./255)
print(model)
acc = test(model, device, test_loader)
print("\nQuantizedDeployable @ mixed-precision accuracy: %.02f%%" % acc)
assert acc >= 99.0

"""The *QuantizedDeployable* network is accurate only in the sense that the operations keep all quantization assumptions. It is not, however, bit-accurate with respect to deployment on an integer-only hardware platform. To get that level of accuracy, we have to transform the network to the last stage: ***IntegerDeployable***.

At this stage, the network can essentially "forget" about the quantum and only work on integer images in all nodes. This means that all weights and activations are replaced by integers! The next cells shows the transformation and how the final integer network still achieves the full accuracy... if we remember that now also test data has to be represented in an integer format.
"""

model = nemo.transform.integerize_pact(model, eps_in=1.0/255)
print(model)
acc = test(model, device, test_loader, integer=True)
print("\nIntegerDeployable @ mixed-precision accuracy: %.02f%%" % acc)
assert acc >= 99.0
